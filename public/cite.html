<!doctype html>
<html lang="en" data-bs-theme="light">

<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>NSF National Deep Inference Fabric</title>
<meta name="description" content="NSF National Deep Inference Fabric" />
<meta property="og:title" content="NSF National Deep Inference Fabric" />
<meta property="og:url" content="https://ndif.us/" />
<meta property="og:image" content="https://ndif.us/images/NDIF_Acr_color.png" />
<meta property="og:description" content="NDIF is a research computing project that enables researchers and students to crack open the mysteries inside large-scale AI systems." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="NSF National Deep Inference Fabric" />
<meta name="twitter:description" content="NDIF is a research computing project that enables researchers and students to crack open the mysteries inside large-scale AI systems." />
<meta name="twitter:image" content="https://ndif.us/images/NDIF_Acr_color.png" />
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/x-icon" href="/favicon.ico">

<!-- Bootstrap CSS Imports -->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">

<!-- Google Font Imports -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap"
rel="stylesheet">

<!-- Custom CSS Imports -->
<link rel="stylesheet" href="./css/home.css">
<link rel="stylesheet" href="./css/custom.css">
<link rel="stylesheet" href="./css/mobile.css">

<!-- Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NQV89E9KBS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-NQV89E9KBS');
</script>

<body>

<header class="pb-3 pt-4">
    <div class="container col-xxl-8 px-5 px-lg-0">
        <div class="row align-items-center">
            <div class="col-4 text-start">
                <a href="./" style="color: inherit; text-decoration: none;">
                    <div class="row align-items-center ">
                        <div class="col-lg-4 border-end no-border-md-down ps-0">
                            <img src="./images/NSF_NDIF_color.png" class="d-block img-fluid" width="120" loading="lazy">
                        </div>
                        <div class="col-md-8 my-auto d-none d-lg-block">
                            <h5 style="margin-bottom: 0 !important; color: #656565; margin-bottom: 0 !important; line-height: 0.9; font-weight: 700 !important;">
                                NSF National Deep<br />Inference Fabric
                            </h6>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-8 align-items-center justify-content-end d-flex">

                <ul class="nav navbar-light p-1 d-none d-md-flex">
                    <li class="nav-item">
                        <a class="nav-link text-secondary" href="./">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link text-secondary" href="./fabric.html">Fabric</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link text-secondary" href="./about.html">About Us</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link text-secondary active" aria-current="page" href="./cite.html">Citing</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link text-secondary" href="./contribute.html">Get Involved</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link text-secondary no-border-md-down border rounded-pill action-button" href="./start.html">Get Started</a>
                    </li>
                </ul>
                
                <button class="navbar-toggler d-md-none" type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvasNavbar" aria-controls="offcanvasNavbar">
                    <span class="navbar-toggler-icon">
                        <i style="font-size: 2rem;" class="bi bi-list"></i>
                    </span>
                </button>

            </div>
        </div>
    </div>

    <div class="offcanvas offcanvas-end" tabindex="-1" id="offcanvasNavbar" aria-labelledby="offcanvasNavbarLabel">
        <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="offcanvasNavbarLabel">Menu</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
        </div>
        <div class="offcanvas-body">
            <ul class="navbar-nav justify-content-end flex-grow-1 pe-3">
                <li><a class="nav-link text-secondary" href="./">Home</a></li>
                <li><a class="nav-link text-secondary" href="./fabric.html">Fabric</a></li>
                <li><a class="nav-link text-secondary" href="./about.html">About Us</a></li>
                <li><a class="nav-link text-secondary active" aria-current="page" href="./cite.html">Citing</a></li>
                <li><a class="nav-link text-secondary" href="./contribute.html">Get Involved</a></li>
                <li><a class="nav-link text-secondary no-border-md-down border rounded-pill action-button" href="./start.html">Get Started</a></li>
            </ul>
        </div>
    </div>
</header>


<main>

  <div class="container  col-xxl-8 px-5 px-lg-0 py-5">
    <div class="row flex-lg-row-reverse align-items-center g-5">
      <div class="col">
        <h1 class="display-5 fw-bold text-body-emphasis lh-1 mb-3">Citing NDIF
        </h1>
      </div>
    </div>
  </div>

  <section class=" col-xxl-8 px-5 px-lg-0 py-5 text-center container border-bottom border-top">

    <div class="row">

      <div class="col-md-12 text-start">
        <p>
        If you use NNsight or NDIF resources in your research, please cite the following:
        </p>
        <div class="card">
        <h3 class="card-header">Citation</h3>
        <div class="card-body">
        <p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect"
        >Jaden Fried Fiotto-Kaufman, Alexander Russell Loftus, Eric Todd, Jannik Brinkmann, 
            Koyena Pal, Dmitrii Troitskii, Michael Ripa, Adam Belfki, Can Rager, Caden Juang, 
            Aaron Mueller, Samuel Marks, Arnab Sen Sharma, Francesca Lucchetti, Nikhil Prakash, 
            Carla E. Brodley, Arjun Guha, Jonathan Bell, Byron C Wallace, and David Bau. "NNsight and NDIF: Democratizing Access to
            Foundation Model Internals," ICLR 2025. Available at
            <a href="https://openreview.net/forum?id=MxbEiFRf39"> https://openreview.net/forum?id=MxbEiFRf39</a>.
        </p>
        </div>
        </div>
        <div class="card">
        <h3 class="card-header">BibTex</h3>
        <div class="card-body">
<pre class="card-text clickselect" style="text-wrap:wrap; overflow:wrap">
<!-- @article{fiotto2024nnsight,
  title={{NNsight} and {NDIF}: Democratizing Access to Foundation Model Internals},
  author={Fiotto-Kaufman, Jaden and Loftus, Alexander R and Todd, Eric and Brinkmann, Jannik and Juang, Caden and Pal, Koyena and Rager, Can and Mueller, Aaron and Marks, Samuel and Sharma, Arnab Sen and Lucchetti, Francesca and Ripa, Michael and Belfki, Adam and Prakash, Nikhil and Multani, Sumeet and Brodley, Carla and Guha, Arjun and Bell, Jonathan and Wallace, Byron and Bau, David},
  journal={arXiv preprint arXiv:2407.14561},
  year={2024}
} -->
@inproceedings{fiotto-kaufman2025nnsight,
  title={{NNsight} and {NDIF}: Democratizing Access to Foundation Model Internals},
  author={Jaden Fried Fiotto-Kaufman and Alexander Russell Loftus and Eric Todd and Jannik Brinkmann and Koyena Pal and Dmitrii Troitskii and Michael Ripa and Adam Belfki and Can Rager and Caden Juang and Aaron Mueller and Samuel Marks and Arnab Sen Sharma and Francesca Lucchetti and Nikhil Prakash and Carla E. Brodley and Arjun Guha and Jonathan Bell and Byron C Wallace and David Bau},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=MxbEiFRf39}
  }
</pre>
        </div>
        </div>
        <p>
        In addition, when you publish work using NNsight or NDIF resources, we'd love you
        to email us directly at <a href="mailto:info@ndif.us">info@ndif.us</a> to tell us about your work. This helps us
        track our impact and supports our continued efforts to provide open-source
        resources for reproducible and transparent research on large-scale AI systems.
        </p>

      </div>
    </div>
  </section>

  <div class="container  col-xxl-8 px-5 px-lg-0 py-5">
    <div class="row flex-lg-row-reverse align-items-center g-5">
      <div class="col">
        <h1 class="display-5 fw-bold text-body-emphasis lh-1 mb-3">Research Using NDIF 
        </h1>
      </div>
    </div>
  </div>


  <section class=" col-xxl-8 px-5 px-lg-0 py-5 container border-bottom border-top" style="font-size: 14px;"> 

    <p>
      <img src="images/DeltaProduct-Improving-State-Tracking-in-Linear-RNNs-via-Householder-Products.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="http://openreview.net/forum?id=nvb60szj5C"
        >Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi.
        DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products.
        <nobr>ICLR 2025 Workshop on Foundation Models in the Wild.</nobr></a>
      <br>
      This paper presents DeltaProduct, a linear RNN that improves expressivity while maintaining efficiency by using a diagonal plus rank-nh transition matrix built from Householder transforms. It outperforms DeltaNet in state tracking, language modeling, and length extrapolation, and the authors also provides new theoretical insights, showing that DeltaNet can solve dihedral group word problems in just two layers.
    </p>

    <p> 
      <img src="images/Robustly-identifying-concepts-introduced-during-chat-fine-tuning-using-crosscoders.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2504.02922"
        >Julian Minder, Clement Dumas, Caden Juang, Bilal Chugtai, Neel Nanda.
        Robustly identifying concepts introduced during chat fine-tuning using crosscoders.
        <nobr>ArXiv 2025.</nobr></a>
      <br>
      Improves model diffing techniques for understanding how fine-tuning alters language model behavior. The authors identify flaws in the standard crosscoder method that lead to misattributing shared concepts as fine-tuning-specific. They introduce Latent Scaling to better measure concept presence across models and propose a new BatchTopK loss that avoids these issues. Their method uncovers chat-specific, interpretable latents (e.g., latents tied to refusals or misinformation) offering clearer insights into the effects of chat tuning.
    </p>
  

    <p>
      <img src="images/The-Dual-Route-Model-of-Induction.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2504.03022"
        >Sheridan Feucht, Eric Todd, Byron Wallace, David Bau.
        The Dual-Route Model of Induction.
        <nobr>ArXiv 2025.</nobr></a>
      <br>
      Identifies concept-level induction heads—attention heads that copy entire lexical units rather than individual tokens. These heads specialize in semantic tasks like translation, while token-level induction heads handle exact copying. The authors show that these two mechanisms operate independently, and ablating token heads leads models to paraphrase instead of copying. They argue concept-level heads may play a broader role in in-context learning.
    </p>
   
    <p> 
      <img src="images/Steering-Fine-Tuning-Generalization-with-Targeted-Concept-Ablation.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/forum?id=2HyKWpAB4i"
        >Helena Casademunt, Caden Juang, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda.
        Steering Fine-Tuning Generalization with Targeted Concept Ablation.
        <nobr>ICLR 2025 Workshop on Building Trust in Language Models and Applications.</nobr></a>
      <br>
      Introduces a method for steering fine-tuned models toward intended generalizations by identifying and ablating sparse autoencoder latents linked to undesired concepts. This helps disambiguate between multiple training-consistent but behaviorally distinct solutions, such as aligned vs. deceptive models. The approach outperforms baselines on two tasks, eliminating spurious gender correlations and guiding attention in double multiple choice, demonstrating its potential for safer model deployment.
   </p>  
   
   
    <p>
      <img src="images/ICLR-In-Context-Learning-of-Representations.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/forum?id=pXlmOmlHJZ"
        >Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Yongyi Yang, Maya Okawa, Kento Nishi, Martin Wattenberg, Hidenori Tanaka.
        ICLR: In-Context Learning of Representations.
        <nobr>ICLR 2025.</nobr></a>
      <br>
      Explores whether large language models can reorganize internal concept representations based on in-context examples that conflict with pretrained semantics. Using a synthetic graph-tracing task, the authors show that sufficient context can trigger a reorganization of representations to match the graph's structure, though strong semantic priors can resist this shift. They interpret the behavior through the lens of energy minimization and argue that context length is a key factor in enabling flexible representation formation.
    </p>
   
   
    <p>
      <img src="images/The-Geometry-of-Refusal-in-Large-Language-Models-Concept-Cones-and-Representational-Independence.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2502.17420"
        >Tom Wollschläger, Jannes Elstner, Simon Geisler, Vincent Cohen-Addad, Stephan Günnemann, Johannes Gasteiger.
        The Geometry of Refusal in Large Language Models: Concept Cones and Representational Independence.
        <nobr>ArXiv 2025.</nobr></a>
      <br>
      Introduces a gradient-based method to identify and analyze internal mechanisms behind refusal in large language models. Challenging prior claims of a single refusal direction, the authors find multiple mechanistically independent directions and multi-dimensional concept structures that govern refusal. They introduce the concept of representational independence to capture both linear and nonlinear intervention effects, revealing that LLM safety behavior relies on more complex internal structures than previously thought.
    </p>
    
    
    <p>
      <img src="images/Structured-In-Context-Task-Representations.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/forum?id=hlOu6w1a8T"
        >Core Francisco Park, Andrew Lee, Ekdeep Singh Lubana, Kento Nishi, Maya Okawa, Hidenori Tanaka.
        Structured In-Context Task Representations.
        <nobr>NeurIPS 2024 Workshop on Symmetry and Geometry in Neural Representations.</nobr></a>
      <br>
      Investigates whether language models develop interpretable internal representations during in-context learning. Using synthetic data based on geometric structures like grids and rings, the authors show that models do form internal representations reflecting these structures. They also find that in-context examples can override existing semantic priors by shaping representations in new dimensions. The study concludes that language models can build meaningful internal representations from in-context examples alone.
    </p>
    
    
    <p>
      <img src="images/Elucidating-Mechanisms-of-Demographic-Bias-in-LLMs-for-Healthcare.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2502.13319"
        >Hiba Ahsan, Arnab Sen Sharma, Silvio Amir, David Bau, Byron C. Wallace.
        Elucidating Mechanisms of Demographic Bias in LLMs for Healthcare.
        <nobr>ArXiv 2025.</nobr></a>
       <br>
       Uncovers how large language models encode sociodemographic information, such as gender and race, within the context of healthcare. The authors find that gender information is concentrated in middle MLP layers and can be manipulated at inference time through patching, impacting clinical vignette generation and predictions related to gender (e.g., depression risk). While race information is more distributed, it can also be influenced. This work represents the first application of interpretability methods to study sociodemographic biases in LLMs for healthcare.
    </p>
    
    
    <p>
      <img src="images/Back-Attention-Understanding-and-Enhancing-Multi-Hop-Reasoning-in-Large-Language-Models.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2502.10835"
        >Zeping Yu, Yonatan Belinkov, Sophia Ananiadou.
        Back Attention: Understanding and Enhancing Multi-Hop Reasoning in Large Language Models.
        <nobr>ArXiv 2025.</nobr></a>
      <br>
      This paper explores how large language models perform multi-hop reasoning using logit flow, a method for tracing how logits evolve across layers during prediction. The authors identify four stages in factual retrieval and find that multi-hop failures often occur during relation attribute extraction. To address this, they propose back attention, which lets lower layers attend to higher-layer hidden states from other positions. This mechanism improves reasoning accuracy across multiple models and datasets.
    </p>
    
    <p>
      <img src="images/Language-Models-Use-Trigonometry-to-Do-Addition.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2502.00873"
        >Subhash Kantamneni, Max Tegmark.
        Language Models Use Trigonometry to Do Addition.
        <nobr>ArXiv 2025.</nobr></a>
      <br>
      Investigates how mid-sized large language models (LLMs) perform mathematical tasks. The authors discover that LLMs represent numbers as a generalized helix which is causally involved in tasks like addition, subtraction, and other arithmetic operations. They propose that LLMs add by manipulating these number-representing helices using the "Clock" algorithm. Through causal interventions and analysis of MLP outputs and attention heads, the authors provide the first representation-level explanation of how LLMs perform mathematical operations.
    </p>

    <p>
      <img src="images/Large-Language-Models-Share-Representations-of-Latent-Grammatical-Concepts-Across-Typologically-Diverse-Languages.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2501.06346"
         >Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller.
         Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages.
         <nobr>ArXiv 2025.</nobr></a>
      <br>
      Investigates how large language models (LLMs) represent grammatical concepts across languages, finding that abstract features like number, gender, and tense are encoded in shared multilingual directions. Using sparse autoencoders and causal interventions, the authors show that ablating these features significantly impairs cross-lingual performance, suggesting that LLMs can develop robust, language-agnostic grammatical abstractions.
    </p>
    
    <p> 
      <img src="images/Hidden-Pieces-An-Analysis-of-Linear-Probes-for-GPT-Representation-Edits.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://ial.eecs.ucf.edu/pdf/Sukthankar-Austin-ICMLA2024.pdf"
        >Austin L Davis, Gita Sukthankar.
        Hidden Pieces: An Analysis of Linear Probes for GPT Representation Edits.
        <nobr>ICMLA 2024.</nobr></a>
      <br>
      This paper explores the use of probing classifiers to analyze and intervene in the internal representations of a chess-playing transformer model. Probing classifiers are small models trained on hidden states to perform specific tasks, serving as tools to reveal and influence how information is encoded—much like neural electrode arrays in neuroscience. The authors demonstrate that the weights of these probes are highly informative and can be used to reliably delete specific pieces from the board, effectively editing the model’s internal game state. This shows that the model learns an emergent, manipulable representation of the chessboard.
    </p>

    <p> 
      <img src="images/Incremental-Sentence-Processing-Mechanisms.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2412.05353"
        >Michael Hanna, Aaron Mueller.
        Incremental Sentence Processing Mechanisms in Autoregressive Transformer Language Models.
        <nobr>ArXiv 2024.</nobr></a>
      <br>
      Investigates how autoregressive language models (LMs) process garden path sentences, revealing that LMs use both syntactic features and shallow heuristics to interpret ambiguous sentences. Using sparse autoencoders, the authors show that LMs simultaneously represent multiple interpretations but do not effectively reanalyze their initial understanding when answering follow-up questions.
    </p>

    <p>
      <img src="images/Separating-Tongue-From-Thought-Activation-Patching.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2411.08745"
          >Clément Dumas, Chris Wendler, Veniamin Veselovsky, Giovanni Monea, Robert West.
          Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers.
          <nobr>ArXiv 2024.</nobr></a>
      <br>
      Investigates whether large language models (LLMs) develop language-agnostic concept representations by analyzing their latent activations during translation tasks. Language information emerges earlier than conceptual meaning in the model's layers and show, through activation patching, that concepts and languages can be independently manipulated, providing evidence for universal concept representations.
    </p>
    
    
    <p>
      <img src="images/Interplm-Discovering-Interpretable-Features-in-Protein-LMs.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://www.biorxiv.org/content/10.1101/2024.11.14.623630v1.abstract"
         >Elana Simon, James Zou.
         InterPLM: Discovering Interpretable Features in Protein Language Models via Sparse Autoencoders.
         <nobr>BiorXiv 2024.</nobr></a>
      <br>
      This paper introduces a method for interpreting protein language models (PLMs) by using sparse autoencoders (SAEs) to extract human-interpretable features from model embeddings. Applied to ESM-2, the approach reveals thousands of latent features per layer that align with known biological concepts like binding sites and structural motifs, far exceeding the interpretability of individual neurons. The authors also identify coherent, novel features that extend beyond current biological annotations and propose a language model-based pipeline to help interpret them. These features can aid in filling database gaps and steering protein design. The study presents InterPLM, a platform for exploring these representations, along with open-source tools for further analysis.
    </p>  
   
   
    <p>
      <img src="images/Token-Erasure-as-a-Footprint-of-Implicit-Vocabulary-Items-in-LLMs.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://aclanthology.org/2024.emnlp-main.543/"
        >Sheridan Feucht, David Atkinson, Byron Wallace, David Bau.
        Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs.
        <nobr>EMNLP 2024.</nobr></a>
      <br>
      Investigates how LLMs transform arbitrary groups of tokens into higher-level representations, focusing on multi-token words and named entities. Identifies a pronounced "erasure" effect where information about previous tokens is quickly forgotten in early layers. Proposes a method to probe the implicit vocabulary of LLMs by analyzing token representation changes across layers, providing results for Llama-2-7b and Llama-3-8B. This study represents the first effort to explore the implicit vocabulary of LLMs.
   </p>


    <p>
      <img src="images/Evidence-of-Learned-Look-Ahead-in-a-Chess-Playing-Neural-Network.png" 
      style="
      width: 300px;
      max-width: 100%;
      height: 100px;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/forum?id=8zg9sO4ttV"
         >Erik Jenner, Shreyas Kapur, Vasil Georgiev, Cameron Allen, Scott Emmons, Stuart Russell.
         Evidence of Learned Look-Ahead in a Chess-Playing Neural Network.
         <nobr>NeurIPS 2024.</nobr></a>
      <br>
      Presents evidence of learned look-ahead in the policy network of Leela Chess Zero, showing that it internally represents future optimal moves, which are critical in certain board states. Demonstrates through activations, attention heads, and a probing model that neural networks can predict optimal moves ahead, providing a basis for understanding learned algorithmic capabilities in neural networks.
    </p>

    <p>
      <img src="images/Representation-Shattering-in-Transformers.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/abs/2410.17194"
         >Kento Nishi, Maya Okawa, Rahul Ramesh, Mikail Khona, Ekdeep Singh Lubana, Hidenori Tanaka.
         Representation Shattering in Transformers: A Synthetic Study with Knowledge Editing.
         <nobr>ArXiv 2024.</nobr></a>
      <br>    
      Investigates why Knowledge Editing (KE) methods in large language models can degrade factual recall and reasoning abilities, proposing that KE inadvertently distorts concept representations beyond the targeted fact—a phenomenon they call representation shattering. Through synthetic tasks and experiments on LLaMA and Mamba models, the authors demonstrate that modifying one fact can disrupt related knowledge structures, explaining the broader performance degradation caused by KE.
    </p>


    <p>
      <img src="images/A-Generative-Benchmark-Creation-Framework.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://dl.acm.org/doi/abs/10.1145/3627673.3679157"
         >Daniel C. Fox, Aamod Khatiwada, Roee Shraga.
         A Generative Benchmark Creation Framework for Detecting Common Data Table Versions.
         <nobr>ACM 2024.</nobr></a>
      <br>
      Introduces a novel framework using large language models (LLMs) to generate benchmarks for data versioning, addressing the lack of standardized evaluation methods in the field. The authors release VerLLM-v1, a benchmark with detailed documentation, version lineage, and complex transformations, facilitating better development and evaluation of data versioning techniques.     
    </p>


    <p>
      <img src="images/Emergence-of-Hierarchical-Emotion-Representations.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/forum?id=vgXUoCrHmp"
        >Bo Zhao, Maya Okawa, Eric J Bigelow, Rose Yu, Tomer Ullman, Hidenori Tanaka.
        Emergence of Hierarchical Emotion Representations in Large Language Models.
        <nobr>NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning.</nobr></a>
      <br>
      Explores how large language models (LLMs) represent and predict human emotions, revealing that larger models develop more complex hierarchical emotion structures and achieve better outcomes in negotiation tasks by accurately modeling counterparts' emotions. Also highlights ethical concerns, showing that LLMs exhibit persona biases, often misclassifying emotions for minority personas, raising important considerations for responsible deployment.     
    </p>

    <p>
      <img src="images/Evaluating-Open-Source-Sparse-Autoencoders-on-Disentangling-Factual-Knowledge-in-GPT-2-Small.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/pdf/2409.04478"
         >Maheep Chaudhary, Atticus Geiger.
         Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small.
         <nobr>ArXiv 2024.</nobr></a>
      <br>
     
      Evaluates the utility of high-dimensional sparse autoencoders (SAEs) for causal analysis in mechanistic 
      interpretability, using the RAVEL benchmark on GPT-2 small. Compares four SAEs to neurons as a baseline 
      and linear features learned via distributed alignment search (DAS) as a skyline. Findings indicate that 
      SAEs struggle to match the neuron baseline and fall significantly short of the DAS skyline in 
      distinguishing between knowledge of a city's country and continent.
    
    </p>
    

    <p>
      <img src="images/Locating-and-Editing-Factual-Associations-in-Mamba.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/pdf?id=yoVRyrEgix"
         >Arnab Sen Sharma, David Atkinson, David Bau.
         Locating and Editing Factual Associations in Mamba.
         <nobr>COLM 2024.</nobr></a>
         <br>
         Investigates factual recall mechanisms in the Mamba state space model, comparing it to autoregressive transformer models. 
         Finds that key components responsible for factual recall are localized in middle layers and at specific token positions,
         mirroring patterns seen in transformers. Demonstrates that rank-one model editing can insert facts at particular locations 
         and adapts attention-knockout techniques to analyze information flow. Despite architectural differences, 
         the study concludes that Mamba and transformer models share significant similarities in factual recall processes.
    </p>

    <p>
      <img src="images/Benchmarking-Mental-State-Representations-in-Language-Models.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/pdf/2406.17513"
         >Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling.
         Benchmarking Mental State Representations in Language Models.
         <nobr>ArXiv 2024.</nobr></a>
         <br>
         Conducts a benchmark study on the internal representation of mental states in language models, 
         analyzing different model sizes, fine-tuning strategies, and prompt designs. Finds that the 
         quality of belief representations improves with model size and fine-tuning but is sensitive to 
         prompt variations. Extends previous activation editing experiments, showing that reasoning 
         performance can be improved by steering model activations without training probes. 
         First to investigate the impact of prompt variations on probing performance in Theory of Mind tasks.
    </p>

    <p>
      <img src="images/Token-Erasure-as-a-Footprint-of-Implicit-Vocabulary-Items-in-LLMs.png" 
      style="
      width: 300px;
      max-width: 100%;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/pdf/2406.20086"
         >Sheridan Feucht, David Atkinson, Byron Wallace, David Bau.
         Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs.
         <nobr>ArXiv 2024.</nobr></a>
         <br>
         Investigates how LLMs transform arbitrary groups of tokens into higher-level representations, 
         focusing on multi-token words and named entities. Identifies a pronounced "erasure" effect where 
         information about previous tokens is quickly forgotten in early layers. Proposes a method to 
         probe the implicit vocabulary of LLMs by analyzing token representation changes across layers, 
         providing results for Llama-2-7b and Llama-3-8B. This study represents the first effort to explore 
         the implicit vocabulary of LLMs.
    </p>


    <p>
      <img src="images/How-do-Llamas-process-multilingual-text-A-latent-exploration-through-activation-patching.png" 
      style="
      width: 300px;
      max-width: 100%;
      height: 150px;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://openreview.net/pdf/4ae31df7b78b2cdab11c6b3f87aa5be7d6cc0701.pdf"
         >Clément Dumas, Veniamin Veselovsky, Giovanni Monea, Robert West, Chris Wendler.
         How do Llamas process multilingual text? A latent exploration through activation patching.
         <nobr>ICML 2024 Workshop on Mechanistic Interpretability.</nobr></a>
         <br> 
         Analyzes Llama-2's forward pass during word translation tasks to explore whether it develops 
         language-agnostic concept representations. Shows that language encoding occurs earlier than 
         concept encoding and that activation patching can independently alter either the concept or 
         the language. Demonstrates that averaging latents across languages does not hinder translation 
         performance, providing evidence for universal concept representation in multilingual models.
    </p>

    <p>
      <img src="images/Language-Models-Represent-Beliefs-of-Self-and-Others.png"
      style="
      width: 300px;
      max-width: 100%;
      height: 150px;
      display: block;
      margin: 0 auto 15px auto;
      float: left;">
      <a href="https://arxiv.org/pdf/2402.18496.pdf"
         >Wentao Zhu, Zhining Zhang, Yizhou Wang.
         Language Models Represent Beliefs of Self and Others.
         <nobr>ICML 2024.</nobr>
      </a>
      <br>
      Investigates the presence of Theory of Mind (ToM) abilities in large language models, 
      identifying internal representations of self and others' beliefs through neural activations. 
      Shows that manipulating these representations significantly alters ToM performance, 
      highlighting their importance in social reasoning. Extends findings to various social 
      reasoning tasks involving causal inference.
    </p>

  </section>



  <div class="container  col-xxl-8 px-5 px-lg-0 py-5">
    <div class="row flex-lg-row-reverse align-items-center g-5">
      <div class="col">
        <h1 class="display-5 fw-bold text-body-emphasis lh-1 mb-3"> Research Referencing NDIF
        </h1>
      </div>
    </div>
  </div>
  
  <section class=" col-xxl-8 px-5 px-lg-0 py-5 container border-bottom border-top" style="font-size: 14px;">


  <p>
    <a href="https://arxiv.org/pdf/2408.01416"
       >Aaron Mueller, Jannik Brinkmann, Millicent Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, Yonatan Belinkov.
       The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability. 
       <nobr>ArXiv 2024.</nobr></a>
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2408.00211"
       >Daniel D. Johnson.
      Penzai + Treescope: A Toolkit for Interpreting, Visualizing, and Editing Models As Data. 
      <nobr>ArXiv 2024.</nobr></a>
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2407.21656"
       >Florian Dietz, Sophie Fellenz, Dietrich Klakow, Marius Kloft.
       Comgra: A Tool for Analyzing and Debugging Neural Networks. 
      <nobr>ArXiv 2024.</nobr></a>
  </p>

  <p>
    <a href="https://dl.acm.org/doi/pdf/10.1145/3630106.3659037"
       >Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall, 
       Andreas Haupt, Kevin Wei, Jérémy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, 
       Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, 
       David Bau, Max Tegmark, David Krueger, Dylan Hadfield-Menell.
       Black-Box Access is Insufficient for Rigorous AI Audits. 
      <nobr>ACM 2024.</nobr></a>
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2405.00208"
       >Javier Ferrando, Gabriele Sarti, Arianna Bisazza, Marta R. Costa-jussà.
       A Primer on the Inner Workings of Transformer-based Language Models. 
      <nobr>ArXiv 2024.</nobr></a>
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2404.07129"
       >Aaditya K. Singh, Ted Moskovitz, Felix Hill, Stephanie C.Y. Chan, Andrew M. Saxe.
       What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation. 
      <nobr>ArXiv 2024.</nobr></a>
  </p>

  <p>
    <a href="https://arxiv.org/pdf/2403.07809"
       >Zhengxuan Wu, Atticus Geiger, Aryaman Arora, Jing Huang, Zheng Wang, Noah D. Goodman, Christopher D. Manning, Christopher Potts.
       Pyvene: A Library for Understanding and Improving PyTorch Models via Interventions. 
      <nobr>ArXiv 2024.</nobr></a>
  </p>

</section>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>

  <script>
  document.addEventListener('click', (e) => {
    if (e.target.matches('.clickselect')) {
      var range = document.createRange();
      range.selectNodeContents(e.target);
      var sel = window.getSelection();
      sel.removeAllRanges();
      sel.addRange(range);
    }
  });
  
  </script>


</main>


<div class="bg-body-secondary px-5 px-lg-0">
    <footer class="container col-xxl-8 py-5 bg-body-secondary">
        <div class="row">
            <div class="col-12 col-md">
                <a href="https://www.northeastern.edu" class="text-decoration-none">
                    <img src="./images/northeastern.svg" alt="Northeastern University" style="width:48px;" class="float-left align-text-top">
                    <small class="d-inline-block text-body-secondary align-text-top mb-2">
                    NDIF is developed at<br>
                    Northeastern University
                    </small>

                </a>
                <small class="d-block mb-3 text-body-secondary">© 2024 NDIF</small>
            </div>
            <div class="col-6 col-md">
                <ul class="list-unstyled text-small">
                    <li><a class="link-secondary text-decoration-none" href="/">Home</a></li>
                    <li><a class="link-secondary text-decoration-none" href="/fabric.html">Fabric</a></li>
                    <li><a class="link-secondary text-decoration-none" href="/about.html">About Us</a></li>
                    <li><a class="link-secondary text-decoration-none" href="/cite.html">Citing NDIF</a></li>
                </ul>
            </div>
            <div class="col-6 col-md">
                <ul class="list-unstyled text-small">
                    <li><a class="link-secondary text-decoration-none" href="/start.html">Get started</a></li>
                    <li><a class="link-secondary text-decoration-none" href="/contribute.html">Get involved</a></li>
                    <li><a class="link-secondary text-decoration-none" href="https://nnsight.net/">NNsight</a></li>
                    <li><a class="link-secondary text-decoration-none" href="/signup.html">Community</a></li>
                </ul>
            </div>
        </div>
    </footer>
</div>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz"
    crossorigin="anonymous">


    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
        
</script>



</body>

</html>

